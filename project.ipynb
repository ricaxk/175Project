{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "NOTICE: Please do not use A100 as GPU type"
      ],
      "metadata": {
        "id": "EcrTfqjBV5Vb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeoLwVZICwcf"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/Unity-Technologies/ml-agents.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!git clone https://github.com/yezy17/175Project.git"
      ],
      "metadata": {
        "id": "pmsTImgQSovL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip3 install -e ml-agents/ml-agents-envs\n",
        "!pip3 install -e ml-agents/ml-agents"
      ],
      "metadata": {
        "id": "mSKyBXuiS3dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!unzip 175Project/RG_env.zip -d env_file"
      ],
      "metadata": {
        "id": "cqhzoSyHS9Kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod -R 755 env_file/RG_env.x86_64\n",
        "!chmod -R 755 env_file/UnityPlayer.so"
      ],
      "metadata": {
        "id": "ntq20j81TeJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mlagents-learn 175Project/config/twoWalkerSelfPlay.yaml --env=env_file/RG_env.x86_64 --run-id=twoWalkerSP_1  --torch-device=\"cuda\" --force --no-graphics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZqT3tgsTpmj",
        "outputId": "c4cc0e67-1dbb-44bf-826c-584e40f9e0a0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "            ┐  ╖\n",
            "        ╓╖╬│╡  ││╬╖╖\n",
            "    ╓╖╬│││││┘  ╬│││││╬╖\n",
            " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
            " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
            " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
            " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
            " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
            " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
            " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
            "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
            "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
            "          ╙╬╬╬╣╣╣╜\n",
            "             ╙\n",
            "        \n",
            " Version information:\n",
            "  ml-agents: 0.31.0.dev0,\n",
            "  ml-agents-envs: 0.31.0.dev0,\n",
            "  Communicator API: 1.5.0,\n",
            "  PyTorch: 1.11.0+cu102\n",
            "[INFO] Connected to Unity environment with package version 2.3.0-exp.4 and communication version 1.5.0\n",
            "[INFO] Connected new brain: testWalker?team=0\n",
            "[INFO] Connected new brain: Guader?team=1\n",
            "2023-06-12 06:39:39.992932: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[INFO] Hyperparameters for behavior name testWalker: \n",
            "\ttrainer_type:\tppo\n",
            "\thyperparameters:\t\n",
            "\t  batch_size:\t2048\n",
            "\t  buffer_size:\t20480\n",
            "\t  learning_rate:\t0.0003\n",
            "\t  beta:\t0.005\n",
            "\t  epsilon:\t0.2\n",
            "\t  lambd:\t0.95\n",
            "\t  num_epoch:\t3\n",
            "\t  shared_critic:\tFalse\n",
            "\t  learning_rate_schedule:\tlinear\n",
            "\t  beta_schedule:\tlinear\n",
            "\t  epsilon_schedule:\tlinear\n",
            "\tcheckpoint_interval:\t500000\n",
            "\tnetwork_settings:\t\n",
            "\t  normalize:\tTrue\n",
            "\t  hidden_units:\t256\n",
            "\t  num_layers:\t3\n",
            "\t  vis_encode_type:\tsimple\n",
            "\t  memory:\tNone\n",
            "\t  goal_conditioning_type:\thyper\n",
            "\t  deterministic:\tFalse\n",
            "\treward_signals:\t\n",
            "\t  extrinsic:\t\n",
            "\t    gamma:\t0.995\n",
            "\t    strength:\t1.0\n",
            "\t    network_settings:\t\n",
            "\t      normalize:\tFalse\n",
            "\t      hidden_units:\t128\n",
            "\t      num_layers:\t2\n",
            "\t      vis_encode_type:\tsimple\n",
            "\t      memory:\tNone\n",
            "\t      goal_conditioning_type:\thyper\n",
            "\t      deterministic:\tFalse\n",
            "\tinit_path:\tNone\n",
            "\tkeep_checkpoints:\t5\n",
            "\teven_checkpoints:\tFalse\n",
            "\tmax_steps:\t55000000\n",
            "\ttime_horizon:\t1000\n",
            "\tsummary_freq:\t30000\n",
            "\tthreaded:\tFalse\n",
            "\tself_play:\t\n",
            "\t  save_steps:\t15000\n",
            "\t  team_change:\t75000\n",
            "\t  swap_steps:\t5000\n",
            "\t  window:\t5\n",
            "\t  play_against_latest_model_ratio:\t0.5\n",
            "\t  initial_elo:\t1200.0\n",
            "\tbehavioral_cloning:\tNone\n",
            "[INFO] Hyperparameters for behavior name Guader: \n",
            "\ttrainer_type:\tppo\n",
            "\thyperparameters:\t\n",
            "\t  batch_size:\t2048\n",
            "\t  buffer_size:\t20480\n",
            "\t  learning_rate:\t0.0003\n",
            "\t  beta:\t0.005\n",
            "\t  epsilon:\t0.2\n",
            "\t  lambd:\t0.95\n",
            "\t  num_epoch:\t3\n",
            "\t  shared_critic:\tFalse\n",
            "\t  learning_rate_schedule:\tlinear\n",
            "\t  beta_schedule:\tlinear\n",
            "\t  epsilon_schedule:\tlinear\n",
            "\tcheckpoint_interval:\t500000\n",
            "\tnetwork_settings:\t\n",
            "\t  normalize:\tTrue\n",
            "\t  hidden_units:\t256\n",
            "\t  num_layers:\t3\n",
            "\t  vis_encode_type:\tsimple\n",
            "\t  memory:\tNone\n",
            "\t  goal_conditioning_type:\thyper\n",
            "\t  deterministic:\tFalse\n",
            "\treward_signals:\t\n",
            "\t  extrinsic:\t\n",
            "\t    gamma:\t0.995\n",
            "\t    strength:\t1.0\n",
            "\t    network_settings:\t\n",
            "\t      normalize:\tFalse\n",
            "\t      hidden_units:\t128\n",
            "\t      num_layers:\t2\n",
            "\t      vis_encode_type:\tsimple\n",
            "\t      memory:\tNone\n",
            "\t      goal_conditioning_type:\thyper\n",
            "\t      deterministic:\tFalse\n",
            "\tinit_path:\tNone\n",
            "\tkeep_checkpoints:\t5\n",
            "\teven_checkpoints:\tFalse\n",
            "\tmax_steps:\t55000000\n",
            "\ttime_horizon:\t1000\n",
            "\tsummary_freq:\t30000\n",
            "\tthreaded:\tFalse\n",
            "\tself_play:\t\n",
            "\t  save_steps:\t15000\n",
            "\t  team_change:\t75000\n",
            "\t  swap_steps:\t5000\n",
            "\t  window:\t5\n",
            "\t  play_against_latest_model_ratio:\t0.5\n",
            "\t  initial_elo:\t1200.0\n",
            "\tbehavioral_cloning:\tNone\n",
            "[INFO] testWalker. Step: 30000. Time Elapsed: 123.462 s. Mean Reward: -0.615. Std of Reward: 0.620. Training. ELO: 964.260.\n",
            "[INFO] testWalker. Step: 60000. Time Elapsed: 247.646 s. Mean Reward: -0.506. Std of Reward: 0.751. Training. ELO: 809.505.\n",
            "[INFO] Guader. Step: 30000. Time Elapsed: 419.624 s. Mean Reward: -0.542. Std of Reward: 0.767. Training. ELO: 832.605.\n",
            "[INFO] Guader. Step: 60000. Time Elapsed: 532.926 s. Mean Reward: -0.360. Std of Reward: 0.987. Training. ELO: 508.250.\n",
            "[INFO] testWalker. Step: 90000. Time Elapsed: 648.556 s. Mean Reward: -0.219. Std of Reward: 1.048. Training. ELO: 769.698.\n",
            "[INFO] testWalker. Step: 120000. Time Elapsed: 760.625 s. Mean Reward: 0.253. Std of Reward: 1.421. Training. ELO: 557.940.\n",
            "[INFO] testWalker. Step: 150000. Time Elapsed: 871.434 s. Mean Reward: 0.821. Std of Reward: 1.863. Training. ELO: 257.875.\n",
            "[INFO] Guader. Step: 90000. Time Elapsed: 927.533 s. Mean Reward: 0.041. Std of Reward: 1.448. Training. ELO: 541.134.\n",
            "[INFO] Learning was interrupted. Please wait while the graph is generated.\n",
            "[INFO] Exported results/twoWalkerSP_1/testWalker/testWalker-150017.onnx\n",
            "[INFO] Copied results/twoWalkerSP_1/testWalker/testWalker-150017.onnx to results/twoWalkerSP_1/testWalker.onnx.\n",
            "[INFO] Exported results/twoWalkerSP_1/Guader/Guader-112822.onnx\n",
            "[INFO] Copied results/twoWalkerSP_1/Guader/Guader-112822.onnx to results/twoWalkerSP_1/Guader.onnx.\n"
          ]
        }
      ]
    }
  ]
}